number of parameters: 0.00M
num decayed parameter tensors: 22, with 2,095 parameters
num non-decayed parameter tensors: 11, with 55 parameters
using fused AdamW: False
Epoch 1/50, Avg Training Loss: 2.081079864501953
Epoch 2/50, Avg Training Loss: 1.823326587677002
Epoch 3/50, Avg Training Loss: 1.5545435070991516
Epoch 4/50, Avg Training Loss: 1.3102636456489563
Epoch 5/50, Avg Training Loss: 1.113695454597473
Epoch 6/50, Avg Training Loss: 0.9723189532756805
Epoch 7/50, Avg Training Loss: 0.8776278078556061
Epoch 8/50, Avg Training Loss: 0.8461476504802704
Epoch 9/50, Avg Training Loss: 0.7933474898338317
Epoch 10/50, Avg Training Loss: 0.7529192507266999
Epoch 11/50, Avg Training Loss: 0.729603385925293
Epoch 12/50, Avg Training Loss: 0.7162060737609863
Epoch 13/50, Avg Training Loss: 0.7014329254627227
Epoch 14/50, Avg Training Loss: 0.6816536962985993
Epoch 15/50, Avg Training Loss: 0.6516200304031372
Epoch 16/50, Avg Training Loss: 0.6630351364612579
Epoch 17/50, Avg Training Loss: 0.6635652363300324
Epoch 18/50, Avg Training Loss: 0.6576592326164246
Epoch 19/50, Avg Training Loss: 0.6337881743907928
Epoch 20/50, Avg Training Loss: 0.6478006720542908
Epoch 21/50, Avg Training Loss: 0.6584425508975983
Epoch 22/50, Avg Training Loss: 0.6433669805526734
Epoch 23/50, Avg Training Loss: 0.6170934796333313
Epoch 24/50, Avg Training Loss: 0.5988585829734803
Epoch 25/50, Avg Training Loss: 0.5818457543849945
Epoch 26/50, Avg Training Loss: 0.5534416854381561
Epoch 27/50, Avg Training Loss: 0.5334988117218018
Epoch 28/50, Avg Training Loss: 0.495957624912262
Epoch 29/50, Avg Training Loss: 0.49471830725669863
Epoch 30/50, Avg Training Loss: 0.4715376317501068
Epoch 31/50, Avg Training Loss: 0.48585336804389956
Epoch 32/50, Avg Training Loss: 0.45869807004928587
Epoch 33/50, Avg Training Loss: 0.44283410012722013
Epoch 34/50, Avg Training Loss: 0.4275937914848328
Epoch 35/50, Avg Training Loss: 0.40712076127529145
Epoch 36/50, Avg Training Loss: 0.3939335376024246
Epoch 37/50, Avg Training Loss: 0.37650099098682405
Epoch 38/50, Avg Training Loss: 0.37801361083984375
Epoch 39/50, Avg Training Loss: 0.368715313076973
Epoch 40/50, Avg Training Loss: 0.3262443393468857
Epoch 41/50, Avg Training Loss: 0.3506605103611946
Epoch 42/50, Avg Training Loss: 0.30017951130867004
Epoch 43/50, Avg Training Loss: 0.2980984538793564
Epoch 44/50, Avg Training Loss: 0.31185510754585266
Epoch 45/50, Avg Training Loss: 0.2784065008163452
Epoch 46/50, Avg Training Loss: 0.26702786833047865
Epoch 47/50, Avg Training Loss: 0.26299073696136477
Epoch 48/50, Avg Training Loss: 0.3114243522286415
Epoch 49/50, Avg Training Loss: 0.27058936804533007
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Epoch 50/50, Avg Training Loss: 0.24846013486385346
Traceback (most recent call last):
  File "/Users/sarahbentley/.julia/conda/3/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/sarahbentley/.julia/conda/3/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/train.py", line 126, in <module>
    main(args)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/train.py", line 116, in main
    by_length = metrics_by_length(model, DFA, [i for i in range(4,args.max_seq_len*2)], batch_size=args.batch_size, pad_idx=0)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/inference_GPT.py", line 56, in metrics_by_length
    y_true, y_pred = infer(model, dataloader, batch_size, same_lengths=True)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/inference_GPT.py", line 27, in infer
    logits, _ = model(pred)
  File "/Users/sarahbentley/dfa_transformer/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/sarahbentley/dfa_transformer/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/GPT.py", line 175, in forward
    assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"
AssertionError: Cannot forward sequence of length 61, block size is only 60