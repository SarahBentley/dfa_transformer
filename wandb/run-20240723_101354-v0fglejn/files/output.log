number of parameters: 0.00M
num decayed parameter tensors: 22, with 2,574 parameters
num non-decayed parameter tensors: 11, with 66 parameters
using fused AdamW: False
Epoch 1/50, Avg Training Loss: 2.050309920310974
Epoch 2/50, Avg Training Loss: 1.7794273734092712
Epoch 3/50, Avg Training Loss: 1.490559458732605
Epoch 4/50, Avg Training Loss: 1.2370267033576965
Epoch 5/50, Avg Training Loss: 1.045473873615265
Epoch 6/50, Avg Training Loss: 0.9343461275100708
Epoch 7/50, Avg Training Loss: 0.8602429032325745
Epoch 8/50, Avg Training Loss: 0.8092771828174591
Epoch 9/50, Avg Training Loss: 0.775181245803833
Epoch 10/50, Avg Training Loss: 0.7658455967903137
Epoch 11/50, Avg Training Loss: 0.7404016494750977
Epoch 12/50, Avg Training Loss: 0.7266699731349945
Epoch 13/50, Avg Training Loss: 0.7134339511394501
Epoch 14/50, Avg Training Loss: 0.7065585553646088
Epoch 15/50, Avg Training Loss: 0.6882823765277862
Epoch 16/50, Avg Training Loss: 0.6830444633960724
Epoch 17/50, Avg Training Loss: 0.6685194671154022
Epoch 18/50, Avg Training Loss: 0.674656742811203
Epoch 19/50, Avg Training Loss: 0.6562607884407043
Epoch 20/50, Avg Training Loss: 0.6514290571212769
Epoch 21/50, Avg Training Loss: 0.6194961845874787
Epoch 22/50, Avg Training Loss: 0.7438498437404633
Epoch 23/50, Avg Training Loss: 0.6564431190490723
Epoch 24/50, Avg Training Loss: 0.6494123876094818
Epoch 25/50, Avg Training Loss: 0.6372367084026337
Epoch 26/50, Avg Training Loss: 0.618551516532898
Epoch 27/50, Avg Training Loss: 0.6191219627857208
Epoch 28/50, Avg Training Loss: 0.6113193571567536
Epoch 29/50, Avg Training Loss: 0.6035829722881317
Epoch 30/50, Avg Training Loss: 0.5637560009956359
Epoch 31/50, Avg Training Loss: 0.5186950862407684
Epoch 32/50, Avg Training Loss: 0.5189400643110276
Epoch 33/50, Avg Training Loss: 0.519164964556694
Epoch 34/50, Avg Training Loss: 0.5011508047580719
Epoch 35/50, Avg Training Loss: 0.5171201944351196
Epoch 36/50, Avg Training Loss: 0.5174996376037597
Epoch 37/50, Avg Training Loss: 0.49629352390766146
Epoch 38/50, Avg Training Loss: 0.4682127356529236
Epoch 39/50, Avg Training Loss: 0.47981812953948977
Epoch 40/50, Avg Training Loss: 0.48336625695228574
Epoch 41/50, Avg Training Loss: 0.47863654494285585
Epoch 42/50, Avg Training Loss: 0.4819487392902374
Epoch 43/50, Avg Training Loss: 0.4951190918684006
Epoch 44/50, Avg Training Loss: 0.46943058967590334
Epoch 45/50, Avg Training Loss: 0.43423381745815276
Epoch 46/50, Avg Training Loss: 0.4296007603406906
Epoch 47/50, Avg Training Loss: 0.4416291147470474
Epoch 48/50, Avg Training Loss: 0.43175436854362487
Epoch 49/50, Avg Training Loss: 0.4408556669950485
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Epoch 50/50, Avg Training Loss: 0.40696210861206056
Traceback (most recent call last):
  File "/Users/sarahbentley/.julia/conda/3/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Users/sarahbentley/.julia/conda/3/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/train.py", line 126, in <module>
    main(args)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/train.py", line 116, in main
    by_length = metrics_by_length(model, DFA, [i for i in range(4,args.max_seq_len*2)], batch_size=args.batch_size, pad_idx=0)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/inference_GPT.py", line 56, in metrics_by_length
    y_true, y_pred = infer(model, dataloader, batch_size, same_lengths=True)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/inference_GPT.py", line 27, in infer
    logits, _ = model(pred)
  File "/Users/sarahbentley/dfa_transformer/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/sarahbentley/dfa_transformer/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/sarahbentley/dfa_transformer/src/GPT/GPT.py", line 175, in forward
    assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"
AssertionError: Cannot forward sequence of length 51, block size is only 50