number of parameters: 0.00M
num decayed parameter tensors: 10, with 1,242 parameters
num non-decayed parameter tensors: 5, with 30 parameters
using fused AdamW: False
Epoch 1/60, Avg Training Loss: 2.1907649993896485
Epoch 2/60, Avg Training Loss: 2.133038640022278
Epoch 3/60, Avg Training Loss: 2.076255750656128
Epoch 4/60, Avg Training Loss: 2.00744971036911
Epoch 5/60, Avg Training Loss: 1.9317644953727722
Epoch 6/60, Avg Training Loss: 1.8581308484077455
Epoch 7/60, Avg Training Loss: 1.7837825655937194
Epoch 8/60, Avg Training Loss: 1.7187833428382873
Epoch 9/60, Avg Training Loss: 1.6528578042984008
Epoch 10/60, Avg Training Loss: 1.5803032755851745
Epoch 11/60, Avg Training Loss: 1.5194653749465943
Epoch 12/60, Avg Training Loss: 1.4585477590560914
Epoch 13/60, Avg Training Loss: 1.3964566826820373
Epoch 14/60, Avg Training Loss: 1.343280053138733
Epoch 15/60, Avg Training Loss: 1.2947224378585815
Epoch 16/60, Avg Training Loss: 1.2450111508369446
Epoch 17/60, Avg Training Loss: 1.2012674689292908
Epoch 18/60, Avg Training Loss: 1.1586868166923523
Epoch 19/60, Avg Training Loss: 1.1274238348007202
Epoch 20/60, Avg Training Loss: 1.0927288055419921
Epoch 21/60, Avg Training Loss: 1.0635130286216736
Epoch 22/60, Avg Training Loss: 1.028277373313904
Epoch 23/60, Avg Training Loss: 1.0057513058185577
Epoch 24/60, Avg Training Loss: 0.9853000462055206
Epoch 25/60, Avg Training Loss: 0.9693196177482605
Epoch 26/60, Avg Training Loss: 0.9479633331298828
Epoch 27/60, Avg Training Loss: 0.9274869561195374
Epoch 28/60, Avg Training Loss: 0.9158935546875
Epoch 29/60, Avg Training Loss: 0.8959994852542877
Epoch 30/60, Avg Training Loss: 0.8745124340057373
Epoch 31/60, Avg Training Loss: 0.8591497659683227
Epoch 32/60, Avg Training Loss: 0.8545546412467957
Epoch 33/60, Avg Training Loss: 0.8382417857646942
Epoch 34/60, Avg Training Loss: 0.8283812820911407
Epoch 35/60, Avg Training Loss: 0.8055278658866882
Epoch 36/60, Avg Training Loss: 0.7901788592338562
Epoch 37/60, Avg Training Loss: 0.7888022541999817
Epoch 38/60, Avg Training Loss: 0.7724484145641327
Epoch 39/60, Avg Training Loss: 0.7543606340885163
Epoch 40/60, Avg Training Loss: 0.7474935293197632
Epoch 41/60, Avg Training Loss: 0.7389710128307343
Epoch 42/60, Avg Training Loss: 0.7242891550064087
Epoch 43/60, Avg Training Loss: 0.7114388704299927
Epoch 44/60, Avg Training Loss: 0.6954978346824646
Epoch 45/60, Avg Training Loss: 0.6876015543937684
Epoch 46/60, Avg Training Loss: 0.6755692780017852
Epoch 47/60, Avg Training Loss: 0.6669867634773254
Epoch 48/60, Avg Training Loss: 0.6612888395786285
Epoch 49/60, Avg Training Loss: 0.6513869106769562
Epoch 50/60, Avg Training Loss: 0.6398682773113251
Epoch 51/60, Avg Training Loss: 0.6273150622844696
Epoch 52/60, Avg Training Loss: 0.6207936346530915
Epoch 53/60, Avg Training Loss: 0.6150355935096741
Epoch 54/60, Avg Training Loss: 0.6131246507167816
Epoch 55/60, Avg Training Loss: 0.6036721110343933
Epoch 56/60, Avg Training Loss: 0.6210854530334473
Epoch 57/60, Avg Training Loss: 0.5997758269309997
Epoch 58/60, Avg Training Loss: 0.5866517126560211
Epoch 59/60, Avg Training Loss: 0.5660010039806366
Epoch 60/60, Avg Training Loss: 0.5522644937038421
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")